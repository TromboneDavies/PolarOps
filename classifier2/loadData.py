# Load the data from TheHandTaggedDataBaby.csv and prepare for classification:
#   - Shuffle data
#   - Split into training and validation sets
#   - Clean text (case, punctuation, etc.; see clean() function and options to
#     CountVectorizer constructor in vectorize() function)
#   - Vectorize according to counts (later: TF/IDF)
#
# The following variables are available after running this file:
#   - ht: the entire shuffled and labeled hand-tagged data set
#   - train: the subset of ht used for training
#   - validate: the fraction of ht used for validation

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import re

# See also https://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-ref
from sklearn.feature_extraction.text import TfidfVectorizer


random.seed(2022)

TRAIN_FRAC = .8     # Fraction of data used for training (not validation)
NGRAM_MAX = 2       # 1 - unigrams only; 2- unigrams+bigrams; etc.



def clean(dirty):
    """Given a real-live Reddit comment thread, prepare it for processing by
    converting to lower-case, removing punctuation, etc.
    """
    dirty = re.sub(r"\(https?[^)]*\)","",dirty)
    dirty = dirty.replace("https://en.wikipedia.org/wiki","")
    dirty = dirty.replace("\\n","")
    return dirty.replace("'","")

def vectorize(train_texts, validate_texts=None, vocab=None):
    """Given a list of (cleaned) texts, and optionally a list of validation
    texts, turn them into matrices of features. This involves both the
    traditional "tokenization" step and occurrence counting.

    The reason train_texts and validate_texts are passed separately is that we
    need to .fit_transform() the former but just .transform() the latter.

    Return values:
      - the array of vectorized training texts: one row per text, and one
        column per feature. Each entry is a number, giving the TF/IDF value
        of that feature in that text.
      - if validate_texts is not None, the second return value is the same as
        above, but for the validation texts.
      - a list of the actual words that each column corresponds to.
      - the dictionary generated by the vectorization process, for use in a
        future call to this function, if desired.
    """
    vectorizer = TfidfVectorizer(
        encoding='utf-8',
        lowercase=True,
        stop_words='english',     # could use None
        ngram_range=(1,NGRAM_MAX),
        analyzer='word',
        min_df=0.0,
        max_df=1.0,
        max_features=None,   # unlimited
        vocabulary=vocab,     # use vocab in the texts
    )
    train_vecs = vectorizer.fit_transform(train_texts).toarray()
    if validate_texts is None:
        return (train_vecs, vectorizer.get_feature_names_out(),
            vectorizer.vocabulary_)
    else:
        validate_vecs = vectorizer.transform(validate_texts).toarray()
        return (train_vecs, validate_vecs, vectorizer.get_feature_names_out(),
            vectorizer.vocabulary_)


ht = pd.read_csv("TheHandTaggedDataBaby.csv", comment="#")
ht.sample(frac=1)   # Shuffle the data before doing anything else.

# Eliminate the top 10% longest threads, since they comprise most of a long,
# probably-not-representative tail. (See email chain 2/18/2022.)
ht = ht[ht.text.str.len() < ht.text.str.len().quantile(.9)]

cleaned_texts = np.empty(len(ht), dtype=object)
for i,row in enumerate(ht.itertuples()):
    cleaned_texts[i] = clean(row.text)
ht['text'] = cleaned_texts

train = ht.sample(frac=TRAIN_FRAC)
validate = ht[~ht.index.isin(train.index)]


